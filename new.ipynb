{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548ef45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\university\\master\\nlp\\hw\\3\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "from gensim.models import Word2Vec, FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "# bash\n",
    "# pip install optuna\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2658ab7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86fe361",
   "metadata": {},
   "source": [
    "part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cead59ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing reviews: 6990280it [05:55, 19665.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 File 'user_restaurant_counts.csv' saved. It contains the number of unique restaurants each user has reviewed.\n",
      "Users: 2121\n",
      "Restaurants: 296\n",
      "Reviews: 23924\n"
     ]
    }
   ],
   "source": [
    "# 1.1\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to check if a business is a restaurant\n",
    "def is_restaurant(categories):\n",
    "    if not isinstance(categories, str):\n",
    "        return False\n",
    "    return 'Restaurants' in categories\n",
    "\n",
    "# Load businesses and filter for restaurants\n",
    "businesses = pd.read_json('yelp_academic_dataset_business.json', lines=True)\n",
    "restaurants = businesses[businesses['categories'].apply(is_restaurant)]\n",
    "restaurant_ids = set(restaurants['business_id'])\n",
    "\n",
    "# Process reviews\n",
    "user_restaurants = defaultdict(set)   # user_id -> set of restaurant IDs reviewed\n",
    "biz_users = defaultdict(set)          # business_id -> set of users who reviewed it\n",
    "valid_reviews = []\n",
    "\n",
    "with open('yelp_academic_dataset_review.json', 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f, desc=\"Processing reviews\"):\n",
    "        review = json.loads(line)\n",
    "        if review['business_id'] in restaurant_ids:\n",
    "            valid_reviews.append(review)\n",
    "            user = review['user_id']\n",
    "            biz = review['business_id']\n",
    "            user_restaurants[user].add(biz)\n",
    "            biz_users[biz].add(user)   # ⬅️ Important fix: collect users for each business\n",
    "\n",
    "# User filter: must have reviewed at least 100 different restaurants\n",
    "qualified_users = {u for u, biz_set in user_restaurants.items() if len(biz_set) >= 100}\n",
    "\n",
    "# Restaurant filter: must be reviewed by at least 1000 unique users\n",
    "qualified_biz = {b for b, user_set in biz_users.items() if len(user_set) >= 1000}\n",
    "\n",
    "# Final review filtering\n",
    "final_reviews = [\n",
    "    r for r in valid_reviews\n",
    "    if r['user_id'] in qualified_users and r['business_id'] in qualified_biz\n",
    "]\n",
    "\n",
    "# Save filtered reviews\n",
    "with open('filtered_reviews.json', 'w', encoding='utf-8') as f:\n",
    "    for r in final_reviews:\n",
    "        f.write(json.dumps(r) + '\\n')\n",
    "\n",
    "# Save number of unique restaurants reviewed per user\n",
    "user_stats = pd.DataFrame([\n",
    "    {'user_id': user, 'unique_restaurant_count': len(biz_ids)}\n",
    "    for user, biz_ids in user_restaurants.items()\n",
    "])\n",
    "\n",
    "# Save to CSV file\n",
    "user_stats.to_csv('user_restaurant_counts.csv', index=False)\n",
    "\n",
    "print(\"📄 File 'user_restaurant_counts.csv' saved. It contains the number of unique restaurants each user has reviewed.\")\n",
    "\n",
    "# Summary report\n",
    "print(\"Users:\", len(qualified_users))\n",
    "print(\"Restaurants:\", len(qualified_biz))\n",
    "print(\"Reviews:\", len(final_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351a4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 20000\n",
      "Validation size: 1962\n",
      "Test size: 1962\n"
     ]
    }
   ],
   "source": [
    "# 1.2\n",
    "filtered_reviews = pd.read_json('filtered_reviews.json', lines=True)\n",
    "\n",
    "# حالا می‌تونی ادامه بدی\n",
    "filtered_reviews['date'] = pd.to_datetime(filtered_reviews['date'])\n",
    "sorted_reviews = filtered_reviews.sort_values('date')\n",
    "\n",
    "train = sorted_reviews.iloc[:20000]\n",
    "rest = sorted_reviews.iloc[20000:]\n",
    "valid = rest.iloc[:len(rest)//2]\n",
    "test = rest.iloc[len(rest)//2:]\n",
    "\n",
    "# ذخیره فایل‌ها\n",
    "train.to_json('train.json', lines=True, orient='records')\n",
    "valid.to_json('valid.json', lines=True, orient='records')\n",
    "test.to_json('test.json', lines=True, orient='records')\n",
    "\n",
    "print(\"Train size:\", len(train))\n",
    "print(\"Validation size:\", len(valid))\n",
    "print(\"Test size:\", len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15ae93",
   "metadata": {},
   "source": [
    "part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268ac7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top 15 Similar Words to 'tasty'\n",
      "  FastText   FT_Sim      Word2Vec  W2V_Sim\n",
      "    tasty- 0.957964         yummy 0.833816\n",
      "     good- 0.839100     delicious 0.813541\n",
      " delicious 0.824517          good 0.807784\n",
      " flavorful 0.821973     flavorful 0.733744\n",
      "      good 0.821693        delish 0.700442\n",
      "delicious- 0.805538    satisfying 0.698690\n",
      "   goodbye 0.793263       filling 0.669034\n",
      "     -good 0.792252 disappointing 0.657894\n",
      "flavorfull 0.790689         bland 0.622572\n",
      "  delicacy 0.790058    refreshing 0.619355\n",
      "     pasty 0.788990        hearty 0.618211\n",
      "flavourful 0.785256   outstanding 0.614990\n",
      "suspicious 0.784993        strong 0.610984\n",
      "  delicous 0.779898      tempting 0.610952\n",
      "     yummy 0.777956   scrumptious 0.605299\n",
      "\n",
      "🔍 Top 15 Similar Words to 'give'\n",
      "FastText   FT_Sim Word2Vec  W2V_Sim\n",
      " forgive 0.823551    bring 0.604657\n",
      "   agave 0.748712 consider 0.583525\n",
      "   given 0.746449    allow 0.577139\n",
      "   gives 0.745981      add 0.566688\n",
      "    gave 0.703354  deserve 0.566082\n",
      "     ive 0.694540     rate 0.563676\n",
      "     ave 0.682584   giving 0.563500\n",
      "    gigi 0.665918  receive 0.557974\n",
      "   suave 0.660015     tell 0.556346\n",
      "    dave 0.650702     take 0.556044\n",
      "    save 0.644228     feed 0.548212\n",
      " receive 0.643520  impress 0.548197\n",
      "    five 0.638373   expand 0.546255\n",
      "    wave 0.635710     help 0.542798\n",
      "   brave 0.634622     lose 0.542133\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2.1\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess(text):\n",
    "    return [w.lower() for w in word_tokenize(text)]\n",
    "\n",
    "# بارگذاری و پیش‌پردازش\n",
    "train_reviews = pd.read_json('train.json', lines=True)\n",
    "sentences = train_reviews['text'].apply(preprocess).tolist()\n",
    "\n",
    "# آموزش مدل‌ها\n",
    "w2v_model = Word2Vec(sentences, seed=1234)\n",
    "ft_model = FastText(sentences, seed=1234)\n",
    "\n",
    "# تابع برای تبدیل لیست مشابه‌ها به DataFrame مرتب\n",
    "def format_similar_words(model, word, topn=15):\n",
    "    similar = model.wv.most_similar(word, topn=topn)\n",
    "    return pd.DataFrame(similar, columns=['Word', 'Similarity'])\n",
    "\n",
    "# ایجاد جدول نهایی\n",
    "df_tasty = pd.concat([\n",
    "    format_similar_words(ft_model, 'tasty').rename(columns={'Word': 'FastText', 'Similarity': 'FT_Sim'}),\n",
    "    format_similar_words(w2v_model, 'tasty').rename(columns={'Word': 'Word2Vec', 'Similarity': 'W2V_Sim'})\n",
    "], axis=1)\n",
    "\n",
    "df_give = pd.concat([\n",
    "    format_similar_words(ft_model, 'give').rename(columns={'Word': 'FastText', 'Similarity': 'FT_Sim'}),\n",
    "    format_similar_words(w2v_model, 'give').rename(columns={'Word': 'Word2Vec', 'Similarity': 'W2V_Sim'})\n",
    "], axis=1)\n",
    "\n",
    "# نمایش زیباتر\n",
    "print(\"\\n🔍 Top 15 Similar Words to 'tasty'\")\n",
    "print(df_tasty.to_string(index=False))\n",
    "\n",
    "print(\"\\n🔍 Top 15 Similar Words to 'give'\")\n",
    "print(df_give.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f20b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top 15 Similar Words to 'tasty'\n",
      "     FastText   FT_Sim      Word2Vec  W2V_Sim\n",
      "       tasty- 0.926242     delicious 0.868959\n",
      "    delicious 0.857806         yummy 0.817968\n",
      "   delicious- 0.837358          good 0.784924\n",
      "    flavorful 0.803806     flavorful 0.772891\n",
      "   flavorfull 0.801029         delic 0.769139\n",
      "        yummy 0.796259      passable 0.745104\n",
      "   flavourful 0.792423  unimpressive 0.736847\n",
      "         good 0.775880  over-dressed 0.724674\n",
      "    delicioso 0.773783 well-prepared 0.721848\n",
      "        good- 0.757103     portioned 0.720370\n",
      "     delicous 0.757069        tastey 0.718796\n",
      "well-flavored 0.757052         good- 0.716557\n",
      "        delic 0.753661   scrumptious 0.716499\n",
      "  deliciously 0.744429        delish 0.714353\n",
      "        yummo 0.743477      absurdly 0.711759\n",
      "\n",
      "🔍 Top 15 Similar Words to 'give'\n",
      " FastText   FT_Sim  Word2Vec  W2V_Sim\n",
      "   giving 0.739561    giving 0.705823\n",
      "  forgive 0.684235      bump 0.703141\n",
      "  deserve 0.681480   deserve 0.672059\n",
      "     gave 0.677911      gave 0.663112\n",
      "forgiving 0.646039  warrants 0.654668\n",
      "     bump 0.639965      ding 0.650211\n",
      "    gives 0.636890 sacrifice 0.633327\n",
      "    given 0.630805      rate 0.628980\n",
      "     rate 0.619094    deduct 0.620970\n",
      "   deduct 0.594132      earn 0.616509\n",
      "     shot 0.591308   elusive 0.596723\n",
      "    bumps 0.589653    redeem 0.596583\n",
      "      gim 0.586058     slack 0.594517\n",
      "deserving 0.584572     loses 0.589777\n",
      "    input 0.578804    4-star 0.583310\n",
      "\n",
      "❓ OOV Example (word: 'givez'):\n",
      "FastText: [ 0.10481171  0.09250528 -0.13057084  0.15348956 -0.05934435]\n",
      "Word2Vec: ❌ OOV not in vocabulary\n"
     ]
    }
   ],
   "source": [
    "# 2.1\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# --- Preprocessing Function ---\n",
    "def preprocess(text):\n",
    "    return [w.lower() for w in word_tokenize(text)]\n",
    "\n",
    "# --- Load and Preprocess Training Reviews ---\n",
    "train_reviews = pd.read_json('train.json', lines=True)\n",
    "sentences = train_reviews['text'].apply(preprocess).tolist()\n",
    "\n",
    "\n",
    "# --- Train Embedding Models ---\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, sg=1, seed=1234)  # SGNS\n",
    "ft_model = FastText(sentences, vector_size=100, window=5, min_count=5, sg=1, seed=1234)   # SGNS with subword info\n",
    "\n",
    "# --- Function to Format Similar Words ---\n",
    "def format_similar_words(model, word, topn=15):\n",
    "    similar = model.wv.most_similar(word, topn=topn)\n",
    "    return pd.DataFrame(similar, columns=['Word', 'Similarity'])\n",
    "\n",
    "# --- Generate Tables for 'tasty' and 'give' ---\n",
    "df_tasty = pd.concat([\n",
    "    format_similar_words(ft_model, 'tasty').rename(columns={'Word': 'FastText', 'Similarity': 'FT_Sim'}),\n",
    "    format_similar_words(w2v_model, 'tasty').rename(columns={'Word': 'Word2Vec', 'Similarity': 'W2V_Sim'})\n",
    "], axis=1)\n",
    "\n",
    "df_give = pd.concat([\n",
    "    format_similar_words(ft_model, 'give').rename(columns={'Word': 'FastText', 'Similarity': 'FT_Sim'}),\n",
    "    format_similar_words(w2v_model, 'give').rename(columns={'Word': 'Word2Vec', 'Similarity': 'W2V_Sim'})\n",
    "], axis=1)\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n🔍 Top 15 Similar Words to 'tasty'\")\n",
    "print(df_tasty.to_string(index=False))\n",
    "\n",
    "print(\"\\n🔍 Top 15 Similar Words to 'give'\")\n",
    "print(df_give.to_string(index=False))\n",
    "\n",
    "# --- Optional: Try OOV Word ---\n",
    "print(\"\\n❓ OOV Example (word: 'givez'):\")\n",
    "try:\n",
    "    print(\"FastText:\", ft_model.wv['givez'][:5])  # shows vector head\n",
    "except KeyError:\n",
    "    print(\"FastText: OOV not handled.\")\n",
    "try:\n",
    "    print(\"Word2Vec:\", w2v_model.wv['givez'][:5])\n",
    "except KeyError:\n",
    "    print(\"Word2Vec: ❌ OOV not in vocabulary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1349e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Document Embedding Function ---\n",
    "def doc_embedding(text, model, aggregation='mean'):\n",
    "    words = preprocess(text)\n",
    "    vectors = [model.wv[w] for w in words if w in model.wv]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    if aggregation == 'sum':\n",
    "        return np.sum(vectors, axis=0)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# مثال استفاده:\n",
    "# X_train = np.array([doc_embedding(text, w2v_model, aggregation='mean') for text in train_reviews['text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95886c6b",
   "metadata": {},
   "source": [
    "paer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a0c9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WORD2VEC-mean] → ridge R² score: 0.3586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35863232612609863"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.1\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "def build_embedding_model(sentences, method='word2vec', **kwargs):\n",
    "    if method == 'word2vec':\n",
    "        model = Word2Vec(sentences, **kwargs)\n",
    "    elif method == 'fasttext':\n",
    "        model = FastText(sentences, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown embedding method\")\n",
    "    return model\n",
    "\n",
    "def tfidf_weighted(text, model, vectorizer):\n",
    "    words = preprocess(text)\n",
    "    vec = vectorizer.transform([text])\n",
    "    vec_size = model.vector_size\n",
    "    result = np.zeros(vec_size)\n",
    "    total_weight = 0\n",
    "    for word in words:\n",
    "        if word in model.wv and word in vectorizer.vocabulary_:\n",
    "            tfidf = vec[0, vectorizer.vocabulary_[word]]\n",
    "            result += model.wv[word] * tfidf\n",
    "            total_weight += tfidf\n",
    "    return result / total_weight if total_weight else result\n",
    "\n",
    "def get_doc_embedding_func(model, aggregation='mean', tfidf_vectorizer=None):\n",
    "    def embed(text):\n",
    "        words = preprocess(text)\n",
    "        if aggregation == 'sum':\n",
    "            vecs = [model.wv[w] for w in words if w in model.wv]\n",
    "            return np.sum(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "        elif aggregation == 'mean':\n",
    "            vecs = [model.wv[w] for w in words if w in model.wv]\n",
    "            return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "        elif aggregation == 'tfidf':\n",
    "            return tfidf_weighted(text, model, tfidf_vectorizer)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown aggregation\")\n",
    "    return embed\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_valid, y_valid):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return r2_score(y_valid, preds)\n",
    "\n",
    "def run_pipeline(embedding_type='word2vec',\n",
    "                 aggregation='mean',\n",
    "                 regression_model='ridge',\n",
    "                 use_scaling=False,\n",
    "                 embedding_params={},\n",
    "                 reg_params={}):\n",
    "    \n",
    "    # Step 1: prepare data\n",
    "    texts = train['text'].tolist()\n",
    "    tokenized = [preprocess(t) for t in texts]\n",
    "    model = build_embedding_model(tokenized, method=embedding_type, **embedding_params)\n",
    "\n",
    "    if aggregation == 'tfidf':\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        vectorizer = TfidfVectorizer(tokenizer=preprocess)\n",
    "        vectorizer.fit(texts)\n",
    "    else:\n",
    "        vectorizer = None\n",
    "\n",
    "    embed_func = get_doc_embedding_func(model, aggregation, vectorizer)\n",
    "\n",
    "    # Step 2: embed train and valid\n",
    "    X_train = np.array([embed_func(t) for t in train['text']])\n",
    "    X_valid = np.array([embed_func(t) for t in valid['text']])\n",
    "    y_train = train['stars'].values\n",
    "    y_valid = valid['stars'].values\n",
    "\n",
    "    # Step 3: Feature scaling\n",
    "    if use_scaling:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "\n",
    "    # Step 4: Choose regressor\n",
    "    if regression_model == 'ridge':\n",
    "        reg = Ridge(**reg_params)\n",
    "    elif regression_model == 'linear':\n",
    "        reg = LinearRegression()\n",
    "    elif regression_model == 'rf':\n",
    "        reg = RandomForestRegressor(**reg_params)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown regression model\")\n",
    "\n",
    "    r2 = evaluate_model(reg, X_train, y_train, X_valid, y_valid)\n",
    "    print(f\"[{embedding_type.upper()}-{aggregation}] → {regression_model} R² score: {r2:.4f}\")\n",
    "    return r2\n",
    "\n",
    "# Example usage\n",
    "run_pipeline(\n",
    "    embedding_type='word2vec',\n",
    "    aggregation='mean',\n",
    "    regression_model='ridge',\n",
    "    embedding_params={'vector_size': 100, 'window': 5, 'sg': 1, 'min_count': 5, 'epochs': 10, 'seed': 1234},\n",
    "    reg_params={'alpha': 1.0},\n",
    "    use_scaling=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc141d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\university\\master\\nlp\\hw\\3\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m use_doc2vec = \u001b[38;5;28;01mFalse\u001b[39;00m           \u001b[38;5;66;03m# True اگر بخوای از doc2vec استفاده کنی\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# آموزش مدل embedding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m embedding_model = train_embedding_model(\u001b[43mtrain_reviews\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtokens\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, method=embedding_type)\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# ساخت ویژگی‌ها\u001b[39;00m\n\u001b[32m     68\u001b[39m X = build_features(train_reviews, embedding_model, method=embedding_type, aggregation=aggregation_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\university\\master\\nlp\\hw\\3\\myenv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\university\\master\\nlp\\hw\\3\\myenv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'tokens'"
     ]
    }
   ],
   "source": [
    "# 3.1\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_embedding_model(sentences, method='word2vec', vector_size=100, window=5, epochs=10, sg=1, seed=1234):\n",
    "    if method == 'word2vec':\n",
    "        model = Word2Vec(sentences, vector_size=vector_size, window=window, epochs=epochs, sg=sg, seed=seed)\n",
    "    elif method == 'fasttext':\n",
    "        model = FastText(sentences, vector_size=vector_size, window=window, epochs=epochs, sg=sg, seed=seed)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid embedding type. Choose 'word2vec' or 'fasttext'\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_doc_embedding(tokens, model, aggregation='mean'):\n",
    "    vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    if aggregation == 'sum':\n",
    "        return np.sum(vectors, axis=0)\n",
    "    elif aggregation == 'mean':\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"Aggregation must be 'mean' or 'sum'\")\n",
    "\n",
    "\n",
    "def train_doc2vec_model(texts, vector_size=100, window=5, dm=1, epochs=20, seed=1234):\n",
    "    tagged_data = [TaggedDocument(words=preprocess(text), tags=[str(i)]) for i, text in enumerate(texts)]\n",
    "    model = Doc2Vec(tagged_data, vector_size=vector_size, window=window, dm=dm, epochs=epochs, seed=seed)\n",
    "    return model\n",
    "\n",
    "def build_features(data, embedding_model, method='word2vec', aggregation='mean', use_doc2vec=False, doc2vec_model=None):\n",
    "    features = []\n",
    "    for i, row in data.iterrows():\n",
    "        tokens = row['tokens']\n",
    "        if use_doc2vec and doc2vec_model:\n",
    "            vec = doc2vec_model.dv[str(i)]\n",
    "        else:\n",
    "            vec = get_doc_embedding(tokens, embedding_model, aggregation=aggregation)\n",
    "        features.append(vec)\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "# --- انتخاب مدل‌های مختلف برای آزمایش ---\n",
    "regressors = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=1234)\n",
    "}\n",
    "\n",
    "# تنظیمات\n",
    "embedding_type = 'word2vec'  # یا 'fasttext'\n",
    "aggregation_type = 'mean'    # یا 'sum'\n",
    "use_doc2vec = False           # True اگر بخوای از doc2vec استفاده کنی\n",
    "\n",
    "# آموزش مدل embedding\n",
    "embedding_model = train_embedding_model(train_reviews['tokens'], method=embedding_type)\n",
    "\n",
    "# ساخت ویژگی‌ها\n",
    "X = build_features(train_reviews, embedding_model, method=embedding_type, aggregation=aggregation_type)\n",
    "\n",
    "# هدف\n",
    "y = train_reviews['stars'].values  # فرض: ستون ستاره‌ها نامش \"stars\" هست\n",
    "\n",
    "# تقسیم‌بندی داده‌ها\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state= 1234)\n",
    "\n",
    "# ارزیابی مدل‌های مختلف\n",
    "for name, reg in regressors.items():\n",
    "    reg.fit(X_train, y_train)\n",
    "    preds = reg.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    print(f\"🔎 {name}: RMSE = {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna_tuning.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec, FastText, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import multiprocessing\n",
    "\n",
    "# ------------------------\n",
    "# Preprocessing\n",
    "# ------------------------\n",
    "def preprocess(text):\n",
    "    return [w.lower() for w in word_tokenize(text)]\n",
    "\n",
    "def build_features(df, model, aggregation='mean'):\n",
    "    features = []\n",
    "    for text in df['text']:\n",
    "        words = preprocess(text)\n",
    "        vectors = [model.wv[w] for w in words if w in model.wv]\n",
    "        if not vectors:\n",
    "            vec = np.zeros(model.vector_size)\n",
    "        elif aggregation == 'sum':\n",
    "            vec = np.sum(vectors, axis=0)\n",
    "        else:\n",
    "            vec = np.mean(vectors, axis=0)\n",
    "        features.append(vec)\n",
    "    return np.array(features)\n",
    "\n",
    "def build_doc2vec_features(df, model):\n",
    "    return np.array([model.infer_vector(preprocess(text)) for text in df['text']])\n",
    "\n",
    "# ------------------------\n",
    "# Objective Template\n",
    "# ------------------------\n",
    "def get_objective(model_type, mode):\n",
    "    def objective(trial):\n",
    "        vector_size = trial.suggest_int('vector_size', 50, 300)\n",
    "        window = trial.suggest_int('window', 2, 10)\n",
    "        epochs = trial.suggest_int('epochs', 5, 30)\n",
    "        min_count = trial.suggest_int('min_count', 1, 10)\n",
    "        aggregation = trial.suggest_categorical('aggregation', ['mean', 'sum'])\n",
    "\n",
    "        if model_type == 'word2vec':\n",
    "            sg = 1 if mode == 'sgns' else 0\n",
    "            model = Word2Vec(\n",
    "                sentences=df['tokens'], vector_size=vector_size,\n",
    "                window=window, epochs=epochs, min_count=min_count,\n",
    "                sg=sg, seed=1234\n",
    "            )\n",
    "            X = build_features(df, model, aggregation)\n",
    "\n",
    "        elif model_type == 'fasttext':\n",
    "            sg = 1 if mode == 'sgns' else 0\n",
    "            model = FastText(\n",
    "                sentences=df['tokens'], vector_size=vector_size,\n",
    "                window=window, epochs=epochs, min_count=min_count,\n",
    "                sg=sg, seed=1234\n",
    "            )\n",
    "            X = build_features(df, model, aggregation)\n",
    "\n",
    "        elif model_type == 'doc2vec':\n",
    "            dm = 1 if mode == 'dm' else 0\n",
    "            tagged = [TaggedDocument(words=toks, tags=[i]) for i, toks in enumerate(df['tokens'])]\n",
    "            model = Doc2Vec(\n",
    "                documents=tagged, vector_size=vector_size, window=window,\n",
    "                epochs=epochs, min_count=min_count, dm=dm, seed=1234\n",
    "            )\n",
    "            X = build_doc2vec_features(df, model)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type\")\n",
    "\n",
    "        y = df['stars'].values\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
    "        reg = Ridge()\n",
    "        reg.fit(X_train, y_train)\n",
    "        y_pred = reg.predict(X_val)\n",
    "        return r2_score(y_val, y_pred)\n",
    "    return objective\n",
    "\n",
    "# ------------------------\n",
    "# Main Execution\n",
    "# ------------------------\n",
    "if __name__ == '__main__':\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    df = pd.read_json(\"train.json\", lines=True)\n",
    "    df['tokens'] = df['text'].apply(preprocess)\n",
    "\n",
    "    configs = [\n",
    "        ('word2vec', 'sgns'),\n",
    "        ('word2vec', 'cbow'),\n",
    "        ('fasttext', 'sgns'),\n",
    "        ('fasttext', 'cbow'),\n",
    "        ('doc2vec', 'dm'),\n",
    "        ('doc2vec', 'dbow')\n",
    "    ]\n",
    "\n",
    "    for model_type, mode in configs:\n",
    "        print(f\"\\n🔧 Optimizing: {model_type.upper()} - {mode.upper()}\")\n",
    "        study = optuna.create_study(direction='maximize', sampler=TPESampler(seed= 1234 ))\n",
    "        objective = get_objective(model_type, mode)\n",
    "        study.optimize(objective, n_trials=50, n_jobs=multiprocessing.cpu_count())\n",
    "\n",
    "        print(\"Best R2:\", study.best_value)\n",
    "        print(\"Best Params:\", study.best_params)\n",
    "\n",
    "        fig = optuna.visualization.matplotlib.plot_param_importances(study)\n",
    "        fig.set_size_inches(8, 5)\n",
    "        fig.savefig(f\"importance_{model_type}_{mode}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5daf4",
   "metadata": {},
   "source": [
    "part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e748b",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "897eb15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model: RandomForest | Embedding: word2vec | Aggregation: mean | RMSE: 0.8326\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "\n",
    "# ---------------------------------\n",
    "# تنظیمات اولیه و فایل‌ها\n",
    "# ---------------------------------\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "embedding_type = 'word2vec'     # 'fasttext' یا 'doc2vec'\n",
    "aggregation_type = 'mean'       # 'mean', 'tfidf', 'doc2vec'\n",
    "regressor_type = 'RandomForest' # 'LinearRegression', 'SVR', 'RandomForest'\n",
    "\n",
    "vector_size = 100\n",
    "window = 5\n",
    "epochs = 20\n",
    "sg = 1  # 1=SGNS, 0=CBOW\n",
    "\n",
    "# ---------------------------------\n",
    "# 1. Load Train Data and Preprocess\n",
    "# ---------------------------------\n",
    "train_df = pd.read_json('train.json', lines=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    return [w.lower() for w in word_tokenize(text)]\n",
    "\n",
    "train_df['tokens'] = train_df['text'].apply(preprocess)\n",
    "\n",
    "# ---------------------------------\n",
    "# 2. Train Word Embedding\n",
    "# ---------------------------------\n",
    "sentences = train_df['tokens'].tolist()\n",
    "\n",
    "def train_embedding(sentences, method='word2vec'):\n",
    "    if method == 'word2vec':\n",
    "        model = Word2Vec(vector_size=vector_size, window=window, sg=sg, seed=SEED, min_count=2)\n",
    "        model.build_vocab(sentences)\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=epochs)\n",
    "    elif method == 'fasttext':\n",
    "        model = FastText(vector_size=vector_size, window=window, sg=sg, seed=SEED, min_count=2)\n",
    "        model.build_vocab(sentences)\n",
    "        model.train(sentences, total_examples=model.corpus_count, epochs=epochs)\n",
    "    else:\n",
    "        model = None\n",
    "    return model\n",
    "\n",
    "if embedding_type in ['word2vec', 'fasttext']:\n",
    "    embedding_model = train_embedding(sentences, method=embedding_type)\n",
    "\n",
    "# ---------------------------------\n",
    "# 3. Document Embedding Aggregation\n",
    "# ---------------------------------\n",
    "def mean_embedding(tokens, model):\n",
    "    vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size)\n",
    "\n",
    "def tfidf_weighted_embedding(df, model):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit(df['text'])\n",
    "    idf_dict = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "    embeddings = []\n",
    "    for tokens in df['tokens']:\n",
    "        weighted = [model.wv[w] * idf_dict.get(w, 0.0) for w in tokens if w in model.wv]\n",
    "        weights = [idf_dict.get(w, 0.0) for w in tokens if w in model.wv]\n",
    "        if weighted:\n",
    "            doc_vec = np.sum(weighted, axis=0) / np.sum(weights)\n",
    "        else:\n",
    "            doc_vec = np.zeros(vector_size)\n",
    "        embeddings.append(doc_vec)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def doc2vec_embedding(df):\n",
    "    tagged = [TaggedDocument(words=t, tags=[str(i)]) for i, t in enumerate(df['tokens'])]\n",
    "    model = Doc2Vec(tagged, vector_size=vector_size, window=window, epochs=epochs, seed=SEED)\n",
    "    return np.array([model.dv[str(i)] for i in range(len(df))])\n",
    "\n",
    "if aggregation_type == 'doc2vec' or embedding_type == 'doc2vec':\n",
    "    X = doc2vec_embedding(train_df)\n",
    "else:\n",
    "    if aggregation_type == 'mean':\n",
    "        X = np.vstack(train_df['tokens'].apply(lambda x: mean_embedding(x, embedding_model)))\n",
    "    elif aggregation_type == 'tfidf':\n",
    "        X = tfidf_weighted_embedding(train_df, embedding_model)\n",
    "\n",
    "y = train_df['stars'].values  # هدف: امتیاز 1 تا 5\n",
    "\n",
    "# ---------------------------------\n",
    "# 4. Feature Manipulation (Optional)\n",
    "# ---------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ---------------------------------\n",
    "# 5. Regression Model Training\n",
    "# ---------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=SEED)\n",
    "\n",
    "regressors = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'SVR': SVR(kernel='rbf', C=1.0),\n",
    "    'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=SEED)\n",
    "}\n",
    "\n",
    "regressor = regressors[regressor_type]\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(f\" Model: {regressor_type} | Embedding: {embedding_type} | Aggregation: {aggregation_type} | RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729d4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 00:36:23,175] A new study created in memory with name: no-name-72f2a10b-78f4-40d5-bfb7-e7ef1fe509e9\n",
      "[I 2025-06-28 00:45:39,841] Trial 0 finished with value: 0.4077486991882324 and parameters: {'vector_size': 150, 'epochs': 13, 'min_count': 1, 'window': 9}. Best is trial 0 with value: 0.4077486991882324.\n",
      "[I 2025-06-28 00:52:01,057] Trial 1 finished with value: 0.43179863691329956 and parameters: {'vector_size': 250, 'epochs': 14, 'min_count': 1, 'window': 4}. Best is trial 1 with value: 0.43179863691329956.\n",
      "[I 2025-06-28 00:57:51,547] Trial 2 finished with value: 0.43681877851486206 and parameters: {'vector_size': 300, 'epochs': 12, 'min_count': 2, 'window': 5}. Best is trial 2 with value: 0.43681877851486206.\n",
      "[I 2025-06-28 01:01:08,969] Trial 3 finished with value: 0.391482949256897 and parameters: {'vector_size': 150, 'epochs': 10, 'min_count': 4, 'window': 4}. Best is trial 2 with value: 0.43681877851486206.\n",
      "[I 2025-06-28 01:05:37,580] Trial 4 finished with value: 0.42100656032562256 and parameters: {'vector_size': 200, 'epochs': 12, 'min_count': 4, 'window': 6}. Best is trial 2 with value: 0.43681877851486206.\n",
      "[I 2025-06-28 01:18:18,609] Trial 5 finished with value: 0.440853476524353 and parameters: {'vector_size': 250, 'epochs': 23, 'min_count': 2, 'window': 7}. Best is trial 5 with value: 0.440853476524353.\n",
      "[I 2025-06-28 01:33:11,689] Trial 6 finished with value: 0.43240469694137573 and parameters: {'vector_size': 200, 'epochs': 28, 'min_count': 3, 'window': 10}. Best is trial 5 with value: 0.440853476524353.\n",
      "[I 2025-06-28 01:39:06,562] Trial 7 finished with value: 0.4413565397262573 and parameters: {'vector_size': 300, 'epochs': 15, 'min_count': 5, 'window': 5}. Best is trial 7 with value: 0.4413565397262573.\n",
      "[I 2025-06-28 01:49:20,912] Trial 8 finished with value: 0.4414205551147461 and parameters: {'vector_size': 250, 'epochs': 30, 'min_count': 4, 'window': 4}. Best is trial 8 with value: 0.4414205551147461.\n",
      "[I 2025-06-28 01:51:59,906] Trial 9 finished with value: 0.3851392865180969 and parameters: {'vector_size': 150, 'epochs': 11, 'min_count': 2, 'window': 3}. Best is trial 8 with value: 0.4414205551147461.\n",
      "[I 2025-06-28 02:01:36,188] Trial 10 finished with value: 0.38921040296554565 and parameters: {'vector_size': 100, 'epochs': 29, 'min_count': 5, 'window': 8}. Best is trial 8 with value: 0.4414205551147461.\n",
      "[I 2025-06-28 02:08:31,062] Trial 11 finished with value: 0.4419282078742981 and parameters: {'vector_size': 300, 'epochs': 18, 'min_count': 5, 'window': 5}. Best is trial 11 with value: 0.4419282078742981.\n",
      "[I 2025-06-28 02:13:44,020] Trial 12 finished with value: 0.44143348932266235 and parameters: {'vector_size': 300, 'epochs': 19, 'min_count': 4, 'window': 3}. Best is trial 11 with value: 0.4419282078742981.\n",
      "[I 2025-06-28 02:18:49,446] Trial 13 finished with value: 0.44384950399398804 and parameters: {'vector_size': 300, 'epochs': 19, 'min_count': 5, 'window': 3}. Best is trial 13 with value: 0.44384950399398804.\n",
      "[I 2025-06-28 02:27:15,679] Trial 14 finished with value: 0.4464215040206909 and parameters: {'vector_size': 300, 'epochs': 19, 'min_count': 5, 'window': 6}. Best is trial 14 with value: 0.4464215040206909.\n",
      "[I 2025-06-28 02:39:00,425] Trial 15 finished with value: 0.441863477230072 and parameters: {'vector_size': 300, 'epochs': 23, 'min_count': 5, 'window': 7}. Best is trial 14 with value: 0.4464215040206909.\n",
      "[I 2025-06-28 02:45:25,774] Trial 16 finished with value: 0.3846670985221863 and parameters: {'vector_size': 100, 'epochs': 23, 'min_count': 3, 'window': 6}. Best is trial 14 with value: 0.4464215040206909.\n",
      "[I 2025-06-28 02:55:06,208] Trial 17 finished with value: 0.44346803426742554 and parameters: {'vector_size': 300, 'epochs': 17, 'min_count': 5, 'window': 8}. Best is trial 14 with value: 0.4464215040206909.\n",
      "[I 2025-06-28 03:00:53,752] Trial 18 finished with value: 0.4464951157569885 and parameters: {'vector_size': 300, 'epochs': 21, 'min_count': 3, 'window': 3}. Best is trial 18 with value: 0.4464951157569885.\n",
      "[I 2025-06-28 03:19:16,184] Trial 19 finished with value: 0.4475190043449402 and parameters: {'vector_size': 300, 'epochs': 26, 'min_count': 3, 'window': 10}. Best is trial 19 with value: 0.4475190043449402.\n",
      "[I 2025-06-28 03:29:56,184] Trial 20 finished with value: 0.3972451686859131 and parameters: {'vector_size': 100, 'epochs': 26, 'min_count': 3, 'window': 10}. Best is trial 19 with value: 0.4475190043449402.\n",
      "[I 2025-06-28 03:46:24,953] Trial 21 finished with value: 0.4520300626754761 and parameters: {'vector_size': 300, 'epochs': 25, 'min_count': 3, 'window': 9}. Best is trial 21 with value: 0.4520300626754761.\n",
      "[I 2025-06-28 04:03:40,891] Trial 22 finished with value: 0.4499848484992981 and parameters: {'vector_size': 300, 'epochs': 26, 'min_count': 3, 'window': 9}. Best is trial 21 with value: 0.4520300626754761.\n",
      "[I 2025-06-28 04:21:08,791] Trial 23 finished with value: 0.4512553811073303 and parameters: {'vector_size': 300, 'epochs': 26, 'min_count': 3, 'window': 9}. Best is trial 21 with value: 0.4520300626754761.\n",
      "[I 2025-06-28 04:35:11,260] Trial 24 finished with value: 0.43374907970428467 and parameters: {'vector_size': 200, 'epochs': 26, 'min_count': 2, 'window': 9}. Best is trial 21 with value: 0.4520300626754761.\n",
      "[I 2025-06-28 04:51:45,823] Trial 25 finished with value: 0.45076870918273926 and parameters: {'vector_size': 300, 'epochs': 25, 'min_count': 3, 'window': 9}. Best is trial 21 with value: 0.4520300626754761.\n",
      "[I 2025-06-28 05:06:31,680] Trial 26 finished with value: 0.4530012011528015 and parameters: {'vector_size': 300, 'epochs': 24, 'min_count': 2, 'window': 8}. Best is trial 26 with value: 0.4530012011528015.\n",
      "[I 2025-06-28 05:24:10,776] Trial 27 finished with value: 0.45654934644699097 and parameters: {'vector_size': 300, 'epochs': 28, 'min_count': 2, 'window': 8}. Best is trial 27 with value: 0.45654934644699097.\n",
      "[I 2025-06-28 05:42:17,376] Trial 28 finished with value: 0.45898503065109253 and parameters: {'vector_size': 300, 'epochs': 28, 'min_count': 1, 'window': 8}. Best is trial 28 with value: 0.45898503065109253.\n",
      "[I 2025-06-28 05:57:41,707] Trial 29 finished with value: 0.4217204451560974 and parameters: {'vector_size': 150, 'epochs': 28, 'min_count': 1, 'window': 8}. Best is trial 28 with value: 0.45898503065109253.\n",
      "[I 2025-06-28 06:08:00,077] Trial 30 finished with value: 0.39241623878479004 and parameters: {'vector_size': 100, 'epochs': 30, 'min_count': 1, 'window': 7}. Best is trial 28 with value: 0.45898503065109253.\n",
      "[I 2025-06-28 06:25:21,557] Trial 31 finished with value: 0.45290690660476685 and parameters: {'vector_size': 300, 'epochs': 28, 'min_count': 2, 'window': 8}. Best is trial 28 with value: 0.45898503065109253.\n",
      "[I 2025-06-28 06:43:31,312] Trial 32 finished with value: 0.4599800109863281 and parameters: {'vector_size': 300, 'epochs': 28, 'min_count': 1, 'window': 8}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 07:02:15,266] Trial 33 finished with value: 0.4481779932975769 and parameters: {'vector_size': 250, 'epochs': 28, 'min_count': 1, 'window': 8}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 07:16:15,937] Trial 34 finished with value: 0.45639652013778687 and parameters: {'vector_size': 300, 'epochs': 24, 'min_count': 1, 'window': 7}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 07:26:32,704] Trial 35 finished with value: 0.4166024923324585 and parameters: {'vector_size': 150, 'epochs': 21, 'min_count': 1, 'window': 7}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 07:39:16,949] Trial 36 finished with value: 0.43872326612472534 and parameters: {'vector_size': 200, 'epochs': 29, 'min_count': 1, 'window': 7}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 07:52:43,922] Trial 37 finished with value: 0.4545316696166992 and parameters: {'vector_size': 300, 'epochs': 27, 'min_count': 1, 'window': 6}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 08:06:32,588] Trial 38 finished with value: 0.4492398500442505 and parameters: {'vector_size': 250, 'epochs': 24, 'min_count': 1, 'window': 7}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 08:24:32,159] Trial 39 finished with value: 0.4569595456123352 and parameters: {'vector_size': 300, 'epochs': 30, 'min_count': 2, 'window': 8}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 08:38:01,623] Trial 40 finished with value: 0.4341199994087219 and parameters: {'vector_size': 200, 'epochs': 30, 'min_count': 2, 'window': 8}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 08:55:00,995] Trial 41 finished with value: 0.4532228112220764 and parameters: {'vector_size': 300, 'epochs': 29, 'min_count': 2, 'window': 8}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 09:11:50,605] Trial 42 finished with value: 0.45987623929977417 and parameters: {'vector_size': 300, 'epochs': 30, 'min_count': 1, 'window': 7}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 09:30:39,891] Trial 43 finished with value: 0.4572333097457886 and parameters: {'vector_size': 300, 'epochs': 30, 'min_count': 1, 'window': 8}. Best is trial 32 with value: 0.4599800109863281.\n",
      "[I 2025-06-28 09:51:21,857] Trial 44 finished with value: 0.46813344955444336 and parameters: {'vector_size': 300, 'epochs': 30, 'min_count': 1, 'window': 9}. Best is trial 44 with value: 0.46813344955444336.\n",
      "[I 2025-06-28 10:13:14,111] Trial 45 finished with value: 0.4550686478614807 and parameters: {'vector_size': 300, 'epochs': 29, 'min_count': 1, 'window': 10}. Best is trial 44 with value: 0.46813344955444336.\n",
      "[I 2025-06-28 10:28:34,281] Trial 46 finished with value: 0.419497549533844 and parameters: {'vector_size': 150, 'epochs': 27, 'min_count': 1, 'window': 9}. Best is trial 44 with value: 0.46813344955444336.\n",
      "[I 2025-06-28 10:49:26,570] Trial 47 finished with value: 0.4522213935852051 and parameters: {'vector_size': 250, 'epochs': 30, 'min_count': 1, 'window': 9}. Best is trial 44 with value: 0.46813344955444336.\n",
      "[I 2025-06-28 11:02:48,578] Trial 48 finished with value: 0.45971572399139404 and parameters: {'vector_size': 300, 'epochs': 27, 'min_count': 1, 'window': 6}. Best is trial 44 with value: 0.46813344955444336.\n",
      "[I 2025-06-28 11:16:26,776] Trial 49 finished with value: 0.4628791809082031 and parameters: {'vector_size': 300, 'epochs': 27, 'min_count': 1, 'window': 6}. Best is trial 44 with value: 0.46813344955444336.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== word2vec-SGNS =====\n",
      "Best trial: FrozenTrial(number=44, state=1, values=[0.46813344955444336], datetime_start=datetime.datetime(2025, 6, 28, 9, 30, 39, 891179), datetime_complete=datetime.datetime(2025, 6, 28, 9, 51, 21, 857919), params={'vector_size': 300, 'epochs': 30, 'min_count': 1, 'window': 9}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'vector_size': CategoricalDistribution(choices=(100, 150, 200, 250, 300)), 'epochs': IntDistribution(high=30, log=False, low=10, step=1), 'min_count': IntDistribution(high=5, log=False, low=1, step=1), 'window': IntDistribution(high=10, log=False, low=3, step=1)}, trial_id=44, value=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 11:16:29,960] A new study created in memory with name: no-name-a6c22af1-1122-401a-911e-8fec781bdc39\n",
      "[I 2025-06-28 11:18:05,664] Trial 0 finished with value: 0.41857314109802246 and parameters: {'vector_size': 150, 'epochs': 13, 'min_count': 1, 'window': 9}. Best is trial 0 with value: 0.41857314109802246.\n",
      "[I 2025-06-28 11:19:49,306] Trial 1 finished with value: 0.47096413373947144 and parameters: {'vector_size': 250, 'epochs': 14, 'min_count': 1, 'window': 4}. Best is trial 1 with value: 0.47096413373947144.\n",
      "[I 2025-06-28 11:21:16,704] Trial 2 finished with value: 0.47289347648620605 and parameters: {'vector_size': 300, 'epochs': 12, 'min_count': 2, 'window': 5}. Best is trial 2 with value: 0.47289347648620605.\n",
      "[I 2025-06-28 11:22:13,914] Trial 3 finished with value: 0.4083157181739807 and parameters: {'vector_size': 150, 'epochs': 10, 'min_count': 4, 'window': 4}. Best is trial 2 with value: 0.47289347648620605.\n",
      "[I 2025-06-28 11:23:24,665] Trial 4 finished with value: 0.45563048124313354 and parameters: {'vector_size': 200, 'epochs': 12, 'min_count': 4, 'window': 6}. Best is trial 2 with value: 0.47289347648620605.\n",
      "[I 2025-06-28 11:26:28,559] Trial 5 finished with value: 0.48284727334976196 and parameters: {'vector_size': 250, 'epochs': 23, 'min_count': 2, 'window': 7}. Best is trial 5 with value: 0.48284727334976196.\n",
      "[I 2025-06-28 11:29:31,416] Trial 6 finished with value: 0.4605831503868103 and parameters: {'vector_size': 200, 'epochs': 28, 'min_count': 3, 'window': 10}. Best is trial 5 with value: 0.48284727334976196.\n",
      "[I 2025-06-28 11:31:17,884] Trial 7 finished with value: 0.487529456615448 and parameters: {'vector_size': 300, 'epochs': 15, 'min_count': 5, 'window': 5}. Best is trial 7 with value: 0.487529456615448.\n",
      "[I 2025-06-28 11:34:40,676] Trial 8 finished with value: 0.48006099462509155 and parameters: {'vector_size': 250, 'epochs': 30, 'min_count': 4, 'window': 4}. Best is trial 7 with value: 0.487529456615448.\n",
      "[I 2025-06-28 11:35:43,411] Trial 9 finished with value: 0.40593385696411133 and parameters: {'vector_size': 150, 'epochs': 11, 'min_count': 2, 'window': 3}. Best is trial 7 with value: 0.487529456615448.\n",
      "[I 2025-06-28 11:38:01,615] Trial 10 finished with value: 0.5054068565368652 and parameters: {'vector_size': 300, 'epochs': 19, 'min_count': 5, 'window': 7}. Best is trial 10 with value: 0.5054068565368652.\n",
      "[I 2025-06-28 11:40:12,443] Trial 11 finished with value: 0.5012789368629456 and parameters: {'vector_size': 300, 'epochs': 18, 'min_count': 5, 'window': 7}. Best is trial 10 with value: 0.5054068565368652.\n",
      "[I 2025-06-28 11:42:32,371] Trial 12 finished with value: 0.5083059072494507 and parameters: {'vector_size': 300, 'epochs': 19, 'min_count': 5, 'window': 8}. Best is trial 12 with value: 0.5083059072494507.\n",
      "[I 2025-06-28 11:44:16,968] Trial 13 finished with value: 0.38719308376312256 and parameters: {'vector_size': 100, 'epochs': 22, 'min_count': 5, 'window': 8}. Best is trial 12 with value: 0.5083059072494507.\n",
      "[I 2025-06-28 11:46:31,341] Trial 14 finished with value: 0.504638671875 and parameters: {'vector_size': 300, 'epochs': 18, 'min_count': 5, 'window': 8}. Best is trial 12 with value: 0.5083059072494507.\n",
      "[I 2025-06-28 11:49:35,076] Trial 15 finished with value: 0.5113236904144287 and parameters: {'vector_size': 300, 'epochs': 24, 'min_count': 4, 'window': 9}. Best is trial 15 with value: 0.5113236904144287.\n",
      "[I 2025-06-28 11:51:42,763] Trial 16 finished with value: 0.3914649486541748 and parameters: {'vector_size': 100, 'epochs': 25, 'min_count': 3, 'window': 10}. Best is trial 15 with value: 0.5113236904144287.\n",
      "[I 2025-06-28 11:54:54,816] Trial 17 finished with value: 0.5087536573410034 and parameters: {'vector_size': 300, 'epochs': 25, 'min_count': 4, 'window': 9}. Best is trial 15 with value: 0.5113236904144287.\n",
      "[I 2025-06-28 11:58:17,045] Trial 18 finished with value: 0.5135455131530762 and parameters: {'vector_size': 300, 'epochs': 26, 'min_count': 3, 'window': 9}. Best is trial 18 with value: 0.5135455131530762.\n",
      "[I 2025-06-28 12:01:49,285] Trial 19 finished with value: 0.515310525894165 and parameters: {'vector_size': 300, 'epochs': 27, 'min_count': 3, 'window': 9}. Best is trial 19 with value: 0.515310525894165.\n",
      "[I 2025-06-28 12:04:10,591] Trial 20 finished with value: 0.39045262336730957 and parameters: {'vector_size': 100, 'epochs': 27, 'min_count': 3, 'window': 10}. Best is trial 19 with value: 0.515310525894165.\n",
      "[I 2025-06-28 12:07:37,819] Trial 21 finished with value: 0.5148285627365112 and parameters: {'vector_size': 300, 'epochs': 26, 'min_count': 3, 'window': 9}. Best is trial 19 with value: 0.515310525894165.\n",
      "[I 2025-06-28 12:11:10,372] Trial 22 finished with value: 0.5114693641662598 and parameters: {'vector_size': 300, 'epochs': 27, 'min_count': 3, 'window': 9}. Best is trial 19 with value: 0.515310525894165.\n",
      "[I 2025-06-28 12:15:05,312] Trial 23 finished with value: 0.5186809301376343 and parameters: {'vector_size': 300, 'epochs': 30, 'min_count': 3, 'window': 8}. Best is trial 23 with value: 0.5186809301376343.\n",
      "[I 2025-06-28 12:18:55,676] Trial 24 finished with value: 0.4586486220359802 and parameters: {'vector_size': 200, 'epochs': 30, 'min_count': 2, 'window': 8}. Best is trial 23 with value: 0.5186809301376343.\n",
      "[I 2025-06-28 12:22:50,385] Trial 25 finished with value: 0.5028924942016602 and parameters: {'vector_size': 300, 'epochs': 29, 'min_count': 3, 'window': 8}. Best is trial 23 with value: 0.5186809301376343.\n",
      "[I 2025-06-28 12:25:59,182] Trial 26 finished with value: 0.5178947448730469 and parameters: {'vector_size': 300, 'epochs': 21, 'min_count': 2, 'window': 10}. Best is trial 23 with value: 0.5186809301376343.\n",
      "[I 2025-06-28 12:29:17,836] Trial 27 finished with value: 0.5047601461410522 and parameters: {'vector_size': 300, 'epochs': 21, 'min_count': 2, 'window': 10}. Best is trial 23 with value: 0.5186809301376343.\n",
      "[I 2025-06-28 12:31:57,699] Trial 28 finished with value: 0.5032999515533447 and parameters: {'vector_size': 300, 'epochs': 16, 'min_count': 1, 'window': 10}. Best is trial 23 with value: 0.5186809301376343.\n",
      "[I 2025-06-28 12:35:04,101] Trial 29 finished with value: 0.43091750144958496 and parameters: {'vector_size': 150, 'epochs': 28, 'min_count': 1, 'window': 6}. Best is trial 23 with value: 0.5186809301376343.\n",
      "[I 2025-06-28 12:37:16,126] Trial 30 finished with value: 0.39665472507476807 and parameters: {'vector_size': 100, 'epochs': 23, 'min_count': 2, 'window': 9}. Best is trial 23 with value: 0.5186809301376343.\n",
      "[I 2025-06-28 12:41:10,987] Trial 31 finished with value: 0.5151950120925903 and parameters: {'vector_size': 300, 'epochs': 26, 'min_count': 3, 'window': 9}. Best is trial 23 with value: 0.5186809301376343.\n",
      "[I 2025-06-28 12:47:07,295] Trial 32 finished with value: 0.522011935710907 and parameters: {'vector_size': 300, 'epochs': 29, 'min_count': 3, 'window': 10}. Best is trial 32 with value: 0.522011935710907.\n",
      "[I 2025-06-28 12:52:49,732] Trial 33 finished with value: 0.4979557991027832 and parameters: {'vector_size': 250, 'epochs': 29, 'min_count': 2, 'window': 10}. Best is trial 32 with value: 0.522011935710907.\n",
      "[I 2025-06-28 12:57:24,122] Trial 34 finished with value: 0.5174839496612549 and parameters: {'vector_size': 300, 'epochs': 30, 'min_count': 3, 'window': 10}. Best is trial 32 with value: 0.522011935710907.\n",
      "[I 2025-06-28 13:00:52,417] Trial 35 finished with value: 0.4285770654678345 and parameters: {'vector_size': 150, 'epochs': 30, 'min_count': 4, 'window': 10}. Best is trial 32 with value: 0.522011935710907.\n",
      "[I 2025-06-28 13:03:52,966] Trial 36 finished with value: 0.46391862630844116 and parameters: {'vector_size': 200, 'epochs': 29, 'min_count': 2, 'window': 10}. Best is trial 32 with value: 0.522011935710907.\n",
      "[I 2025-06-28 13:07:55,499] Trial 37 finished with value: 0.51740962266922 and parameters: {'vector_size': 300, 'epochs': 28, 'min_count': 1, 'window': 10}. Best is trial 32 with value: 0.522011935710907.\n",
      "[I 2025-06-28 13:11:51,608] Trial 38 finished with value: 0.48510873317718506 and parameters: {'vector_size': 250, 'epochs': 30, 'min_count': 4, 'window': 6}. Best is trial 32 with value: 0.522011935710907.\n",
      "[I 2025-06-28 13:14:16,783] Trial 39 finished with value: 0.5070962905883789 and parameters: {'vector_size': 300, 'epochs': 21, 'min_count': 3, 'window': 5}. Best is trial 32 with value: 0.522011935710907.\n"
     ]
    }
   ],
   "source": [
    "# optuna_hyperparameter_tuning.py\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from gensim.models import Word2Vec, FastText, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Preprocess\n",
    "train_reviews = pd.read_json('train.json', lines=True)\n",
    "valid_reviews = pd.read_json('valid.json', lines=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    return [w.lower() for w in word_tokenize(text)]\n",
    "\n",
    "sentences = train_reviews['text'].apply(preprocess).tolist()\n",
    "val_texts = valid_reviews['text'].apply(preprocess)\n",
    "y_val = valid_reviews['stars'].values\n",
    "\n",
    "# Word2Vec & FastText Embedding Function\n",
    "def embed_docs(model, docs, vector_size):\n",
    "    result = []\n",
    "    for tokens in docs:\n",
    "        vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "        result.append(np.mean(vectors, axis=0) if vectors else np.zeros(vector_size))\n",
    "    return np.array(result)\n",
    "\n",
    "# Doc2Vec Embedding Function\n",
    "def embed_docs_d2v(model, docs, vector_size):\n",
    "    return np.array([model.infer_vector(toks) for toks in docs])\n",
    "\n",
    "# Template for study\n",
    "\n",
    "def run_study(model_type):\n",
    "    def objective(trial):\n",
    "        vector_size = trial.suggest_categorical(\"vector_size\", [100, 150, 200, 250, 300])\n",
    "        epochs = trial.suggest_int(\"epochs\", 10, 30)\n",
    "        min_count = trial.suggest_int(\"min_count\", 1, 5)\n",
    "\n",
    "        if model_type.startswith(\"word2vec\") or model_type.startswith(\"fastText\"):\n",
    "            window = trial.suggest_int(\"window\", 3, 10)\n",
    "            sg = 1 if \"SGNS\" in model_type else 0\n",
    "\n",
    "            if model_type.startswith(\"word2vec\"):\n",
    "                model = Word2Vec(sentences, vector_size=vector_size, window=window, \n",
    "                                 epochs=epochs, min_count=min_count, sg=sg, seed=SEED)\n",
    "            else:\n",
    "                min_n = trial.suggest_int(\"min_n\", 2, 4)\n",
    "                max_n = trial.suggest_int(\"max_n\", 5, 6)\n",
    "                model = FastText(sentences, vector_size=vector_size, window=window, \n",
    "                                 epochs=epochs, min_count=min_count, sg=sg, \n",
    "                                 min_n=min_n, max_n=max_n, seed=SEED)\n",
    "\n",
    "            X = embed_docs(model, val_texts, vector_size)\n",
    "\n",
    "        elif model_type.startswith(\"doc2vec\"):\n",
    "            tagged_docs = [TaggedDocument(words=t, tags=[i]) for i, t in enumerate(sentences)]\n",
    "            dm = 1 if \"DM\" in model_type else 0\n",
    "            model = Doc2Vec(tagged_docs, vector_size=vector_size, epochs=epochs, \n",
    "                            min_count=min_count, dm=dm, seed=SEED)\n",
    "            X = embed_docs_d2v(model, val_texts, vector_size)\n",
    "\n",
    "        reg = Ridge()\n",
    "        reg.fit(X, y_val)\n",
    "        preds = reg.predict(X)\n",
    "        return r2_score(y_val, preds)\n",
    "\n",
    "    sampler = TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    print(f\"\\n===== {model_type} =====\")\n",
    "    print(\"Best trial:\", study.best_trial)\n",
    "    try:\n",
    "        import optuna.visualization as vis\n",
    "        fig = vis.plot_param_importances(study)\n",
    "        fig.show()\n",
    "    except:\n",
    "        pass\n",
    "    return study\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    settings = [\n",
    "        'word2vec-SGNS', 'word2vec-CBOW',\n",
    "        'fastText-SGNS', 'fastText-CBOW',\n",
    "        'doc2vec-DM', 'doc2vec-DBOW'\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "    for setting in settings:\n",
    "        results[setting] = run_study(setting)\n",
    "\n",
    "    # Optional: Save studies\n",
    "    for name, study in results.items():\n",
    "        study.trials_dataframe().to_csv(f'{name}_optuna_trials.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c171c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\university\\master\\nlp\\hw\\3\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-06-28 16:32:49,077] A new study created in memory with name: no-name-33f02c8b-de1a-4e34-acd6-f5e75d7dff2d\n",
      "[I 2025-06-28 16:42:45,544] Trial 0 finished with value: 0.5433725575800599 and parameters: {'vector_size': 250, 'epochs': 15, 'min_count': 2, 'window': 9}. Best is trial 0 with value: 0.5433725575800599.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== word2vec-SGNS =====\n",
      "Best trial: FrozenTrial(number=0, state=1, values=[0.5433725575800599], datetime_start=datetime.datetime(2025, 6, 28, 16, 32, 49, 94888), datetime_complete=datetime.datetime(2025, 6, 28, 16, 42, 45, 544643), params={'vector_size': 250, 'epochs': 15, 'min_count': 2, 'window': 9}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'vector_size': CategoricalDistribution(choices=(100, 150, 200, 250, 300)), 'epochs': IntDistribution(high=30, log=False, low=10, step=1), 'min_count': IntDistribution(high=5, log=False, low=1, step=1), 'window': IntDistribution(high=10, log=False, low=3, step=1)}, trial_id=0, value=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 16:42:48,027] A new study created in memory with name: no-name-fa2002f1-fc10-448f-b5b0-ef54c0e59140\n",
      "[I 2025-06-28 16:45:18,698] Trial 0 finished with value: 0.539526243525664 and parameters: {'vector_size': 250, 'epochs': 15, 'min_count': 2, 'window': 9}. Best is trial 0 with value: 0.539526243525664.\n",
      "[I 2025-06-28 16:45:18,698] A new study created in memory with name: no-name-516fed96-7bd4-4490-a109-37763308064a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== word2vec-CBOW =====\n",
      "Best trial: FrozenTrial(number=0, state=1, values=[0.539526243525664], datetime_start=datetime.datetime(2025, 6, 28, 16, 42, 48, 43753), datetime_complete=datetime.datetime(2025, 6, 28, 16, 45, 18, 698860), params={'vector_size': 250, 'epochs': 15, 'min_count': 2, 'window': 9}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'vector_size': CategoricalDistribution(choices=(100, 150, 200, 250, 300)), 'epochs': IntDistribution(high=30, log=False, low=10, step=1), 'min_count': IntDistribution(high=5, log=False, low=1, step=1), 'window': IntDistribution(high=10, log=False, low=3, step=1)}, trial_id=0, value=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 17:04:21,046] Trial 0 finished with value: 0.5462658916667551 and parameters: {'vector_size': 250, 'epochs': 15, 'min_count': 2, 'window': 9, 'min_n': 4, 'max_n': 6}. Best is trial 0 with value: 0.5462658916667551.\n",
      "[I 2025-06-28 17:04:21,052] A new study created in memory with name: no-name-c7f71af0-bf68-4691-9636-878d3b12c609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== fastText-SGNS =====\n",
      "Best trial: FrozenTrial(number=0, state=1, values=[0.5462658916667551], datetime_start=datetime.datetime(2025, 6, 28, 16, 45, 18, 698860), datetime_complete=datetime.datetime(2025, 6, 28, 17, 4, 21, 46447), params={'vector_size': 250, 'epochs': 15, 'min_count': 2, 'window': 9, 'min_n': 4, 'max_n': 6}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'vector_size': CategoricalDistribution(choices=(100, 150, 200, 250, 300)), 'epochs': IntDistribution(high=30, log=False, low=10, step=1), 'min_count': IntDistribution(high=5, log=False, low=1, step=1), 'window': IntDistribution(high=10, log=False, low=3, step=1), 'min_n': IntDistribution(high=4, log=False, low=2, step=1), 'max_n': IntDistribution(high=6, log=False, low=5, step=1)}, trial_id=0, value=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 17:15:58,268] Trial 0 finished with value: 0.4692528567840909 and parameters: {'vector_size': 250, 'epochs': 15, 'min_count': 2, 'window': 9, 'min_n': 4, 'max_n': 6}. Best is trial 0 with value: 0.4692528567840909.\n",
      "[I 2025-06-28 17:15:58,271] A new study created in memory with name: no-name-d313bdba-3365-4b54-800e-8acfb09995ff\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== fastText-CBOW =====\n",
      "Best trial: FrozenTrial(number=0, state=1, values=[0.4692528567840909], datetime_start=datetime.datetime(2025, 6, 28, 17, 4, 21, 54450), datetime_complete=datetime.datetime(2025, 6, 28, 17, 15, 58, 267711), params={'vector_size': 250, 'epochs': 15, 'min_count': 2, 'window': 9, 'min_n': 4, 'max_n': 6}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'vector_size': CategoricalDistribution(choices=(100, 150, 200, 250, 300)), 'epochs': IntDistribution(high=30, log=False, low=10, step=1), 'min_count': IntDistribution(high=5, log=False, low=1, step=1), 'window': IntDistribution(high=10, log=False, low=3, step=1), 'min_n': IntDistribution(high=4, log=False, low=2, step=1), 'max_n': IntDistribution(high=6, log=False, low=5, step=1)}, trial_id=0, value=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 17:19:58,380] Trial 0 finished with value: 0.39839579446547113 and parameters: {'vector_size': 250, 'epochs': 15, 'min_count': 2}. Best is trial 0 with value: 0.39839579446547113.\n",
      "[I 2025-06-28 17:19:58,384] A new study created in memory with name: no-name-37697dca-40a6-40aa-84cf-4e8612e3a832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== doc2vec-DM =====\n",
      "Best trial: FrozenTrial(number=0, state=1, values=[0.39839579446547113], datetime_start=datetime.datetime(2025, 6, 28, 17, 15, 58, 272703), datetime_complete=datetime.datetime(2025, 6, 28, 17, 19, 58, 380108), params={'vector_size': 250, 'epochs': 15, 'min_count': 2}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'vector_size': CategoricalDistribution(choices=(100, 150, 200, 250, 300)), 'epochs': IntDistribution(high=30, log=False, low=10, step=1), 'min_count': IntDistribution(high=5, log=False, low=1, step=1)}, trial_id=0, value=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 17:22:38,938] Trial 0 finished with value: 0.45027486687144924 and parameters: {'vector_size': 250, 'epochs': 15, 'min_count': 2}. Best is trial 0 with value: 0.45027486687144924.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== doc2vec-DBOW =====\n",
      "Best trial: FrozenTrial(number=0, state=1, values=[0.45027486687144924], datetime_start=datetime.datetime(2025, 6, 28, 17, 19, 58, 386114), datetime_complete=datetime.datetime(2025, 6, 28, 17, 22, 38, 938821), params={'vector_size': 250, 'epochs': 15, 'min_count': 2}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'vector_size': CategoricalDistribution(choices=(100, 150, 200, 250, 300)), 'epochs': IntDistribution(high=30, log=False, low=10, step=1), 'min_count': IntDistribution(high=5, log=False, low=1, step=1)}, trial_id=0, value=None)\n"
     ]
    }
   ],
   "source": [
    "# test 1 trial\n",
    "# optuna_hyperparameter_tuning.py\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from gensim.models import Word2Vec, FastText, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "# Preprocess\n",
    "train_reviews = pd.read_json('train.json', lines=True)\n",
    "valid_reviews = pd.read_json('valid.json', lines=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    return [w.lower() for w in word_tokenize(text)]\n",
    "\n",
    "sentences = train_reviews['text'].apply(preprocess).tolist()\n",
    "val_texts = valid_reviews['text'].apply(preprocess)\n",
    "y_val = valid_reviews['stars'].values\n",
    "\n",
    "# Word2Vec & FastText Embedding Function\n",
    "def embed_docs(model, docs, vector_size):\n",
    "    result = []\n",
    "    for tokens in docs:\n",
    "        vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "        result.append(np.mean(vectors, axis=0) if vectors else np.zeros(vector_size))\n",
    "    return np.array(result)\n",
    "\n",
    "# Doc2Vec Embedding Function\n",
    "def embed_docs_d2v(model, docs, vector_size):\n",
    "    return np.array([model.infer_vector(toks) for toks in docs])\n",
    "\n",
    "# Template for study\n",
    "\n",
    "def run_study(model_type):\n",
    "    def objective(trial):\n",
    "        vector_size = trial.suggest_categorical(\"vector_size\", [100, 150, 200, 250, 300])\n",
    "        epochs = trial.suggest_int(\"epochs\", 10, 30)\n",
    "        min_count = trial.suggest_int(\"min_count\", 1, 5)\n",
    "\n",
    "        if model_type.startswith(\"word2vec\") or model_type.startswith(\"fastText\"):\n",
    "            window = trial.suggest_int(\"window\", 3, 10)\n",
    "            sg = 1 if \"SGNS\" in model_type else 0\n",
    "\n",
    "            if model_type.startswith(\"word2vec\"):\n",
    "                model = Word2Vec(sentences, vector_size=vector_size, window=window, \n",
    "                                 epochs=epochs, min_count=min_count, sg=sg, seed=SEED)\n",
    "            else:\n",
    "                min_n = trial.suggest_int(\"min_n\", 2, 4)\n",
    "                max_n = trial.suggest_int(\"max_n\", 5, 6)\n",
    "                model = FastText(sentences, vector_size=vector_size, window=window, \n",
    "                                 epochs=epochs, min_count=min_count, sg=sg, \n",
    "                                 min_n=min_n, max_n=max_n, seed=SEED)\n",
    "\n",
    "            X = embed_docs(model, val_texts, vector_size)\n",
    "\n",
    "        elif model_type.startswith(\"doc2vec\"):\n",
    "            tagged_docs = [TaggedDocument(words=t, tags=[i]) for i, t in enumerate(sentences)]\n",
    "            dm = 1 if \"DM\" in model_type else 0\n",
    "            model = Doc2Vec(tagged_docs, vector_size=vector_size, epochs=epochs, \n",
    "                            min_count=min_count, dm=dm, seed=SEED)\n",
    "            X = embed_docs_d2v(model, val_texts, vector_size)\n",
    "\n",
    "        reg = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=SEED)\n",
    "        reg.fit(X, y_val)\n",
    "        preds = reg.predict(X)\n",
    "        return r2_score(y_val, preds)\n",
    "\n",
    "    sampler = TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=1)\n",
    "    print(f\"\\n===== {model_type} =====\")\n",
    "    print(\"Best trial:\", study.best_trial)\n",
    "    try:\n",
    "        import optuna.visualization as vis\n",
    "        fig = vis.plot_param_importances(study)\n",
    "        fig.show()\n",
    "    except:\n",
    "        pass\n",
    "    return study\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    settings = [\n",
    "        'word2vec-SGNS', 'word2vec-CBOW',\n",
    "        'fastText-SGNS', 'fastText-CBOW',\n",
    "        'doc2vec-DM', 'doc2vec-DBOW'\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "    for setting in settings:\n",
    "        results[setting] = run_study(setting)\n",
    "\n",
    "    # Optional: Save studies\n",
    "    for name, study in results.items():\n",
    "        study.trials_dataframe().to_csv(f'{name}_optuna_trials.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834984d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 20:27:04,660] A new study created in memory with name: no-name-b32acb13-c13d-4f85-b370-652144b27ffe\n",
      "[I 2025-06-28 20:50:14,719] Trial 0 finished with value: 0.5220784989700811 and parameters: {'vector_size': 150, 'epochs': 17, 'min_count': 2, 'window': 4}. Best is trial 0 with value: 0.5220784989700811.\n",
      "[I 2025-06-28 21:02:21,995] Trial 2 finished with value: 0.5517016801320288 and parameters: {'vector_size': 300, 'epochs': 12, 'min_count': 1, 'window': 9}. Best is trial 2 with value: 0.5517016801320288.\n",
      "[I 2025-06-28 21:03:53,277] Trial 3 finished with value: 0.5265013118366382 and parameters: {'vector_size': 300, 'epochs': 14, 'min_count': 4, 'window': 9}. Best is trial 2 with value: 0.5517016801320288.\n",
      "[I 2025-06-28 21:08:19,134] Trial 4 finished with value: 0.5153044969624514 and parameters: {'vector_size': 250, 'epochs': 15, 'min_count': 3, 'window': 3}. Best is trial 2 with value: 0.5517016801320288.\n",
      "[I 2025-06-28 21:10:12,623] Trial 1 finished with value: 0.4939772547602669 and parameters: {'vector_size': 100, 'epochs': 25, 'min_count': 1, 'window': 9}. Best is trial 2 with value: 0.5517016801320288.\n",
      "[I 2025-06-28 21:19:50,259] Trial 5 finished with value: 0.47323718179247865 and parameters: {'vector_size': 100, 'epochs': 20, 'min_count': 5, 'window': 4}. Best is trial 2 with value: 0.5517016801320288.\n",
      "[I 2025-06-28 21:26:43,420] Trial 7 finished with value: 0.5125128194980806 and parameters: {'vector_size': 200, 'epochs': 12, 'min_count': 5, 'window': 6}. Best is trial 2 with value: 0.5517016801320288.\n",
      "[I 2025-06-28 21:32:19,271] Trial 8 finished with value: 0.5126613598565772 and parameters: {'vector_size': 100, 'epochs': 19, 'min_count': 4, 'window': 7}. Best is trial 2 with value: 0.5517016801320288.\n",
      "[I 2025-06-28 21:34:09,784] Trial 6 finished with value: 0.5355767904668565 and parameters: {'vector_size': 150, 'epochs': 30, 'min_count': 5, 'window': 4}. Best is trial 2 with value: 0.5517016801320288.\n",
      "[I 2025-06-28 21:35:10,683] Trial 9 finished with value: 0.5101744908697373 and parameters: {'vector_size': 200, 'epochs': 24, 'min_count': 2, 'window': 3}. Best is trial 2 with value: 0.5517016801320288.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== word2vec-SGNS =====\n",
      "Best trial: FrozenTrial(number=2, state=1, values=[0.5517016801320288], datetime_start=datetime.datetime(2025, 6, 28, 20, 27, 4, 737889), datetime_complete=datetime.datetime(2025, 6, 28, 21, 2, 21, 995943), params={'vector_size': 300, 'epochs': 12, 'min_count': 1, 'window': 9}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'vector_size': CategoricalDistribution(choices=(100, 150, 200, 250, 300)), 'epochs': IntDistribution(high=30, log=False, low=10, step=1), 'min_count': IntDistribution(high=5, log=False, low=1, step=1), 'window': IntDistribution(high=10, log=False, low=3, step=1)}, trial_id=2, value=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-28 21:35:11,173] A new study created in memory with name: no-name-ea92a809-ba95-421e-9eb4-cf702846514d\n",
      "[I 2025-06-28 21:41:33,300] Trial 0 finished with value: 0.5332306365266628 and parameters: {'vector_size': 200, 'epochs': 11, 'min_count': 3, 'window': 7}. Best is trial 0 with value: 0.5332306365266628.\n",
      "[I 2025-06-28 21:41:43,535] Trial 1 finished with value: 0.5135399307075488 and parameters: {'vector_size': 200, 'epochs': 11, 'min_count': 1, 'window': 9}. Best is trial 0 with value: 0.5332306365266628.\n",
      "[I 2025-06-28 21:44:27,644] Trial 3 finished with value: 0.5232170879261189 and parameters: {'vector_size': 200, 'epochs': 20, 'min_count': 2, 'window': 8}. Best is trial 0 with value: 0.5332306365266628.\n",
      "[I 2025-06-28 21:44:42,638] Trial 2 finished with value: 0.5187079655193989 and parameters: {'vector_size': 250, 'epochs': 17, 'min_count': 4, 'window': 7}. Best is trial 0 with value: 0.5332306365266628.\n",
      "[I 2025-06-28 21:50:21,462] Trial 7 finished with value: 0.4484876782809758 and parameters: {'vector_size': 100, 'epochs': 13, 'min_count': 2, 'window': 3}. Best is trial 0 with value: 0.5332306365266628.\n",
      "[I 2025-06-28 21:51:13,437] Trial 5 finished with value: 0.49965616950332636 and parameters: {'vector_size': 150, 'epochs': 21, 'min_count': 3, 'window': 10}. Best is trial 0 with value: 0.5332306365266628.\n",
      "[I 2025-06-28 21:51:16,804] Trial 4 finished with value: 0.490885164727906 and parameters: {'vector_size': 200, 'epochs': 25, 'min_count': 3, 'window': 4}. Best is trial 0 with value: 0.5332306365266628.\n",
      "[I 2025-06-28 21:53:49,656] Trial 6 finished with value: 0.5134719518435865 and parameters: {'vector_size': 200, 'epochs': 22, 'min_count': 5, 'window': 10}. Best is trial 0 with value: 0.5332306365266628.\n"
     ]
    }
   ],
   "source": [
    "# test 1 trial\n",
    "# optuna_hyperparameter_tuning.py\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from gensim.models import Word2Vec, FastText, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "# Preprocess\n",
    "train_reviews = pd.read_json('train.json', lines=True)\n",
    "valid_reviews = pd.read_json('valid.json', lines=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    return [w.lower() for w in word_tokenize(text)]\n",
    "\n",
    "sentences = train_reviews['text'].apply(preprocess).tolist()\n",
    "val_texts = valid_reviews['text'].apply(preprocess)\n",
    "y_val = valid_reviews['stars'].values\n",
    "\n",
    "# Word2Vec & FastText Embedding Function\n",
    "def embed_docs(model, docs, vector_size):\n",
    "    result = []\n",
    "    for tokens in docs:\n",
    "        vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "        result.append(np.mean(vectors, axis=0) if vectors else np.zeros(vector_size))\n",
    "    return np.array(result)\n",
    "\n",
    "# Doc2Vec Embedding Function\n",
    "def embed_docs_d2v(model, docs, vector_size):\n",
    "    return np.array([model.infer_vector(toks) for toks in docs])\n",
    "\n",
    "# Template for study\n",
    "\n",
    "def run_study(model_type):\n",
    "    def objective(trial):\n",
    "        vector_size = trial.suggest_categorical(\"vector_size\", [100, 150, 200, 250, 300])\n",
    "        epochs = trial.suggest_int(\"epochs\", 10, 30)\n",
    "        min_count = trial.suggest_int(\"min_count\", 1, 5)\n",
    "\n",
    "        if model_type.startswith(\"word2vec\") or model_type.startswith(\"fastText\"):\n",
    "            window = trial.suggest_int(\"window\", 3, 10)\n",
    "            sg = 1 if \"SGNS\" in model_type else 0\n",
    "\n",
    "            if model_type.startswith(\"word2vec\"):\n",
    "                model = Word2Vec(sentences, vector_size=vector_size, window=window, \n",
    "                                 epochs=epochs, min_count=min_count, sg=sg, seed=SEED)\n",
    "            else:\n",
    "                min_n = trial.suggest_int(\"min_n\", 2, 4)\n",
    "                max_n = trial.suggest_int(\"max_n\", 5, 6)\n",
    "                model = FastText(sentences, vector_size=vector_size, window=window, \n",
    "                                 epochs=epochs, min_count=min_count, sg=sg, \n",
    "                                 min_n=min_n, max_n=max_n, seed=SEED)\n",
    "\n",
    "            X = embed_docs(model, val_texts, vector_size)\n",
    "\n",
    "        elif model_type.startswith(\"doc2vec\"):\n",
    "            tagged_docs = [TaggedDocument(words=t, tags=[i]) for i, t in enumerate(sentences)]\n",
    "            dm = 1 if \"DM\" in model_type else 0\n",
    "            model = Doc2Vec(tagged_docs, vector_size=vector_size, epochs=epochs, \n",
    "                            min_count=min_count, dm=dm, seed=SEED)\n",
    "            X = embed_docs_d2v(model, val_texts, vector_size)\n",
    "\n",
    "        reg = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=SEED)\n",
    "        reg.fit(X, y_val)\n",
    "        preds = reg.predict(X)\n",
    "        return r2_score(y_val, preds)\n",
    "\n",
    "    sampler = TPESampler(seed=SEED)\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "\n",
    "    # n_trials رو بیشتر کن که چند trial داشته باشی\n",
    "    # n_jobs رو بزار بیشتر از 1 تا از multiprocessing داخلی Optuna استفاده کنه\n",
    "    study.optimize(objective, n_trials=10, n_jobs=4)  # مثلا 4 تا پردازش همزمان\n",
    "\n",
    "    print(f\"\\n===== {model_type} =====\")\n",
    "    print(\"Best trial:\", study.best_trial)\n",
    "    try:\n",
    "        import optuna.visualization as vis\n",
    "        fig = vis.plot_param_importances(study)\n",
    "        fig.show()\n",
    "    except:\n",
    "        pass\n",
    "    return study\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    settings = [\n",
    "        'word2vec-SGNS', 'word2vec-CBOW',\n",
    "        'fastText-SGNS', 'fastText-CBOW',\n",
    "        'doc2vec-DM', 'doc2vec-DBOW'\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "    for setting in settings:\n",
    "        results[setting] = run_study(setting)\n",
    "\n",
    "    # Optional: Save studies\n",
    "    for name, study in results.items():\n",
    "        study.trials_dataframe().to_csv(f'{name}_optuna_trials.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
